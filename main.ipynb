{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e1fd34e-d0e1-4830-a186-3dde06db4d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562425d-148d-4830-8976-e66a26a8664c",
   "metadata": {},
   "source": [
    "**Download Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25426141-c1ee-4096-a642-4a45f866e094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True,)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37adb9d-c48d-417a-b2b8-cc0af4912f1e",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca473a25-95c0-430e-bda9-bf70db456892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer, clean=True):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    \n",
    "    Takes a string argument and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. All tokens are mapped to seg-\n",
    "    ment id = 1.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be converted\n",
    "        tokenizer (obj): Tokenizer object\n",
    "            to convert text into BERT-re-\n",
    "            adable tokens and ids\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BERT-readable tokens\n",
    "        obj: Torch tensor with token ids\n",
    "        obj: Torch tensor segment ids\n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    if clean:\n",
    "        tokenized_text = marked_text.split(\" \")\n",
    "    else:\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokenized_text, tokens_tensor, segments_tensors\n",
    "\n",
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "    \"\"\"Get embeddings from an embedding model\n",
    "    \n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns:\n",
    "        list: List of list of floats of size\n",
    "            [n_tokens, n_embedding_dimensions]\n",
    "            containing embeddings for each token\n",
    "    \"\"\"\n",
    "    \n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Removing the first hidden state\n",
    "        # The first state is the input state\n",
    "        hidden_states = outputs[2][1:]\n",
    "\n",
    "    # Getting embeddings from the final BERT layer\n",
    "    token_embeddings = hidden_states[-1]\n",
    "    # Collapsing the tensor into 1-dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "    # Converting torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "    return list_token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38f3a3-479d-47f9-85d4-ff6b9fedea8b",
   "metadata": {},
   "source": [
    "**Read in Corpus & Define Topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47e2f3e7-8213-44b9-899f-44ec7da906de",
   "metadata": {},
   "outputs": [],
   "source": [
    "virus_type = [\"COVID-19\", \"sars\", \"MERS\", \"Ebola\"]\n",
    "virus_study = [\"origin\", \"evolution\", \"symptom\", \"examination\"]\n",
    "age_group = [\"infant\", \"adult\", \"elderly\"]\n",
    "\n",
    "all_words = [\"COVID-19\", \"sars\", \"MERS\", \"Ebola\", \"origin\", \"evolution\", \"symptom\", \"examination\", \"infant\", \"adult\", \"elderly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea50b34-3876-41a3-813e-7861d08e5f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/shared/data2/pk36/multidim/covid_phrase_text.txt') as f:\n",
    "    text = f.readlines()\n",
    "text = text[0]\n",
    "\n",
    "split_text = text.split(\".\")\n",
    "\n",
    "word = \"sars\"\n",
    "split_text.insert(0, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f6bb3-f63f-4713-8ef8-f73ee5b007a7",
   "metadata": {},
   "source": [
    "**Compute Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b93ebe3-14b6-4199-be91-431f70397e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word_embeddings = []\n",
    "\n",
    "for t in split_text:\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(t, tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "    \n",
    "    if word not in tokenized_text:\n",
    "        continue;\n",
    "    else:\n",
    "        word_index = tokenized_text.index(word)\n",
    "        # Get the embedding for bank\n",
    "        word_embedding = list_token_embeddings[word_index]\n",
    "        target_word_embeddings.append(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8f3a914-0275-48d2-a1c9-3f4120759c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9e1e6-ad2e-471a-8daa-6b7f5fe53994",
   "metadata": {},
   "source": [
    "**Cosine-Similiarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "670c7c83-84c0-42d8-bf40-fe1a6685cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the distance between the embeddings of the word in all the given contexts of the word\n",
    "\n",
    "list_of_distances = []\n",
    "for text1, embed1 in zip(split_text, target_word_embeddings):\n",
    "    for text2, embed2 in zip(split_text, target_word_embeddings):\n",
    "        cos_dist = 1 - cosine(embed1, embed2)\n",
    "        list_of_distances.append([text1, text2, cos_dist])\n",
    "\n",
    "distances_df = pd.DataFrame(list_of_distances, columns=['text1', 'text2', 'distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e7fe99b-6079-45c2-939c-ed76a1064199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sars</td>\n",
       "      <td>sars</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sars</td>\n",
       "      <td>angiotensin_converting_enzyme 2 ( ace2 ) as a ...</td>\n",
       "      <td>0.615406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sars</td>\n",
       "      <td>there is a diversity of possible intermediate...</td>\n",
       "      <td>0.602366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sars</td>\n",
       "      <td>5 identity in amino_acid sequences 6 and , im...</td>\n",
       "      <td>0.547345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sars</td>\n",
       "      <td>this similarity with sars cov is critical bec...</td>\n",
       "      <td>0.531670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sars</td>\n",
       "      <td>it is required for host_cell entry and subseq...</td>\n",
       "      <td>0.527674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sars</td>\n",
       "      <td>wan et al</td>\n",
       "      <td>0.520226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sars</td>\n",
       "      <td>4 reported that residue 394 ( glutamine ) in ...</td>\n",
       "      <td>0.517760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sars</td>\n",
       "      <td>thus , the sars_cov_2 spike_protein was predi...</td>\n",
       "      <td>0.512084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sars</td>\n",
       "      <td>further analysis even suggested that sars_cov...</td>\n",
       "      <td>0.509995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text1                                              text2  distance\n",
       "0   sars                                               sars  1.000000\n",
       "1   sars  angiotensin_converting_enzyme 2 ( ace2 ) as a ...  0.615406\n",
       "3   sars   there is a diversity of possible intermediate...  0.602366\n",
       "9   sars   5 identity in amino_acid sequences 6 and , im...  0.547345\n",
       "14  sars   this similarity with sars cov is critical bec...  0.531670\n",
       "15  sars   it is required for host_cell entry and subseq...  0.527674\n",
       "10  sars                                         wan et al   0.520226\n",
       "11  sars   4 reported that residue 394 ( glutamine ) in ...  0.517760\n",
       "13  sars   thus , the sars_cov_2 spike_protein was predi...  0.512084\n",
       "12  sars   further analysis even suggested that sars_cov...  0.509995"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df = distances_df[distances_df.text1 == word].sort_values(by=['distance'], ascending=False)\n",
    "word_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0b7bc71-0427-451f-a856-a5b5cfe46a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' a phylogenetic analysis 3 , 4 found a bat origin for the sars_cov_2 '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df[\"text2\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae5183-6872-455e-8565-d0a0954c2211",
   "metadata": {},
   "source": [
    "**Scratch Work (aka ignore)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16d44256-ae23-450a-8cf7-1445fcf092f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ang', '##iot', '##ens', '##in', '_', 'converting', '_', 'enzyme', '2', '(', 'ace', '##2', ')', 'as', 'a', 'sar', '##s', '_', 'co', '##v', '_', '2', 'receptor', 'molecular', '_', 'mechanisms', 'and', 'potential', 'therapeutic', '_', 'target', 'sar', '##s', '_', 'co', '##v', '_', '2', 'has', 'been', 'sequence', '##d', '3', '.', 'a', 'phylogenetic', 'analysis', '3', ',', '4', 'found', 'a', 'bat', 'origin', 'for', 'the', 'sar', '##s', '_', 'co', '##v', '_', '2', '.', 'there', 'is', 'a', 'diversity', 'of', 'possible', 'intermediate', '_', 'hosts', 'for', 'sar', '##s', '_', 'co', '##v', '_', '2', ',', 'including', 'pang', '##olin', '##s', ',', 'but', 'not', 'mice', 'and', 'rats', '5', '.', 'there', 'are', 'many', 'similarities', 'of', 'sar', '##s', '_', 'co', '##v', '_', '2', 'with', 'the', 'original', 'sar', '##s', 'co', '##v', '.', 'using', 'computer', 'modeling', ',', 'xu', 'et', 'al', '.', '6', 'found', 'that', 'the', 'spike', '_', 'proteins', 'of', 'sar', '##s', '_', 'co', '##v', '_', '2', 'and', 'sar', '##s', 'co', '##v', 'have', '[SEP]']\n",
      "0 : not found\n",
      "512 : not found\n",
      "1024 : not found\n",
      "1536 : not found\n",
      "2048 : not found\n",
      "2560 : not found\n",
      "3072 : not found\n",
      "3584 : not found\n",
      "4096 : not found\n",
      "4608 : not found\n",
      "5120 : not found\n",
      "5632 : not found\n",
      "6144 : not found\n",
      "6656 : not found\n",
      "7168 : not found\n",
      "7680 : not found\n",
      "8192 : not found\n",
      "8704 : not found\n",
      "9216 : not found\n"
     ]
    }
   ],
   "source": [
    "target_word_embeddings = []\n",
    "\n",
    "for i in np.arange(0, len(text[0]), 512):\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text[0][i:min(i+512, len(text[0]))], tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "\n",
    "    word = \"covid_19\" # virus_type[1]\n",
    "\n",
    "    # Find the position 'bank' in list of tokens\n",
    "    if word not in tokenized_text:\n",
    "        if i == 0:\n",
    "            print(tokenized_text)\n",
    "        print(i, \": not found\")\n",
    "    else:\n",
    "        print(i, \": found\")\n",
    "        word_index = tokenized_text.index(word)\n",
    "        # Get the embedding for bank\n",
    "        word_embedding = list_token_embeddings[word_index]\n",
    "        target_word_embeddings.append(word_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schemaegc",
   "language": "python",
   "name": "schemaegc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
