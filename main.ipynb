{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e1fd34e-d0e1-4830-a186-3dde06db4d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/schemaegc/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import torch\n",
    "import numba\n",
    "import math\n",
    "from numba import jit, cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562425d-148d-4830-8976-e66a26a8664c",
   "metadata": {},
   "source": [
    "**Download Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25426141-c1ee-4096-a642-4a45f866e094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True,)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37adb9d-c48d-417a-b2b8-cc0af4912f1e",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca473a25-95c0-430e-bda9-bf70db456892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer, clean=True):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    \n",
    "    Takes a string argument and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. All tokens are mapped to seg-\n",
    "    ment id = 1.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be converted\n",
    "        tokenizer (obj): Tokenizer object\n",
    "            to convert text into BERT-re-\n",
    "            adable tokens and ids\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BERT-readable tokens\n",
    "        obj: Torch tensor with token ids\n",
    "        obj: Torch tensor segment ids\n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    if clean:\n",
    "        tokenized_text = marked_text.split(\" \")\n",
    "    else:\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokenized_text, tokens_tensor, segments_tensors\n",
    "\n",
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "    \"\"\"Get embeddings from an embedding model\n",
    "    \n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns:\n",
    "        list: List of list of floats of size\n",
    "            [n_tokens, n_embedding_dimensions]\n",
    "            containing embeddings for each token\n",
    "    \"\"\"\n",
    "    \n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Removing the first hidden state\n",
    "        # The first state is the input state\n",
    "        hidden_states = outputs[2][1:]\n",
    "\n",
    "    # Getting embeddings from the final BERT layer\n",
    "    token_embeddings = hidden_states[-1]\n",
    "    # Collapsing the tensor into 1-dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "    # Converting torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "    return list_token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1571e1cb-04f1-4e17-8c8b-f3318bd5799e",
   "metadata": {},
   "source": [
    "**Split the Dataset in 512 Chunks & Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47e2f3e7-8213-44b9-899f-44ec7da906de",
   "metadata": {},
   "outputs": [],
   "source": [
    "virus_type = [\"COVID-19\", \"sars\", \"MERS\", \"Ebola\"]\n",
    "virus_study = [\"origin\", \"evolution\", \"symptom\", \"examination\"]\n",
    "age_group = [\"infant\", \"adult\", \"elderly\"]\n",
    "\n",
    "#all_topics = [\"covid_19\", \"sars\", \"mers\", \"ebola\", \"origin\", \"evolution\", \"symptom\", \"examination\", \"infant\", \"adult\", \"elderly\"]\n",
    "all_topics = [\"sars\"]\n",
    "topics_added = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e68675-0dfb-4361-91d4-7171db679551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunks(corpus, size=512, limit=None): #TODO: CHANGE TO GENERATOR and might need to introduce overlap\n",
    "    # corpus is an array of strings where each item represents a document\n",
    "    for doc in tqdm(corpus):\n",
    "        if num == limit:\n",
    "            break\n",
    "        sents = doc.split(\".\")\n",
    "        for s in sents:\n",
    "            marked_s = \"[CLS] \" + s + \" [SEP]\"\n",
    "            split_sent = marked_s.split(\" \")\n",
    "        \n",
    "        ret_txt = np.array_split(split_sent, math.ceil(tokenized_text.shape[0]/size))\n",
    "        if not topics_added:\n",
    "            token_topics = [np.array([\"[CLS]\", w, \"[SEP]\"], dtype=str) for w in all_topics]\n",
    "            ret_txt[:0] = token_topics\n",
    "            topics_added = True\n",
    "        \n",
    "    \n",
    "    return np.array(ret_txt, dtype=np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "740e1cdc-03b0-4f90-b191-db447a29c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkize(corpus, size=512, limit=None): #TODO: CHANGE TO GENERATOR and might need to introduce overlap\n",
    "    tokenized_text = None\n",
    "    num = 0\n",
    "    # corpus is an array of strings where each item represents a document\n",
    "    for doc in tqdm(corpus):\n",
    "        if num == limit:\n",
    "            break\n",
    "        sents = doc.split(\".\")\n",
    "        for s in sents:\n",
    "            marked_s = \"[CLS] \" + s + \" [SEP]\"\n",
    "            split_sent = marked_s.split(\" \")\n",
    "            if tokenized_text is None:\n",
    "                tokenized_text = split_sent\n",
    "            else:\n",
    "                tokenized_text = np.append(tokenized_text, split_sent)\n",
    "            \n",
    "        num += 1\n",
    "    print(tokenized_text.shape)\n",
    "    ret_txt = np.array_split(tokenized_text, math.ceil(tokenized_text.shape[0]/size))\n",
    "    token_topics = [np.array([\"[CLS]\", w, \"[SEP]\"], dtype=str) for w in all_topics]\n",
    "    ret_txt[:0] = token_topics\n",
    "    \n",
    "    return np.array(ret_txt, dtype=np.ndarray)\n",
    "\n",
    "def tokenize(chunk):        \n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(chunk)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokens_tensor, segments_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38f3a3-479d-47f9-85d4-ff6b9fedea8b",
   "metadata": {},
   "source": [
    "**Read in Corpus & Define Topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea50b34-3876-41a3-813e-7861d08e5f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/shared/data2/pk36/multidim/covid_phrase_text.txt') as f:\n",
    "    docs = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48595bf8-78bd-4fdd-baa7-7a2a753864fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 100/29500 [02:20<11:26:03,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286384,)\n",
      "(561,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunks = chunkize(docs, limit=100)\n",
    "print(chunks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f6bb3-f63f-4713-8ef8-f73ee5b007a7",
   "metadata": {},
   "source": [
    "**Compute Embeddings for All Words in Each Chunk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1087e75c-db33-4545-93d7-a89bb87d935f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 561/561 [04:53<00:00,  1.91it/s]\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = None # shape: (# words, 768)\n",
    "all_words = []\n",
    "add_topics = True\n",
    "\n",
    "for chunk in tqdm(chunks):\n",
    "    tokens_tensor, segments_tensors = tokenize(chunk)\n",
    "    list_token_embeddings = np.array(get_bert_embeddings(tokens_tensor, segments_tensors, model)) # shape: (512 tokens in chunk, 768)\n",
    "    all_words.extend(chunk)\n",
    "\n",
    "    if word_embeddings is None:\n",
    "        word_embeddings = list_token_embeddings\n",
    "    else:\n",
    "        word_embeddings = np.append(word_embeddings, list_token_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "564ab0f7-8c0e-4f2f-bdbf-c03f7dcc1ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286387"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12970ee7-5e02-441a-b767-3b2a7561ccdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286387, 768)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9e1e6-ad2e-471a-8daa-6b7f5fe53994",
   "metadata": {},
   "source": [
    "**Cosine-Similiarity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ffc97-7e8e-4f9a-a91c-192b06af4a26",
   "metadata": {},
   "source": [
    "_Word-to-Word:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2c55944-992f-4fac-9a7a-6720eab147e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "topic_col = []\n",
    "word_col = []\n",
    "cos_col = []\n",
    "\n",
    "for i in tqdm(np.arange(len(all_topics))):\n",
    "    topic_col.extend(np.repeat(all_topics[i], len(all_words)))\n",
    "    word_col.extend(all_words)\n",
    "    cos_col.extend(cosine_similarity(word_embeddings[i].reshape(1, -1), word_embeddings).reshape(-1, 1))\n",
    "\n",
    "    \n",
    "topic_col = np.array(topic_col)\n",
    "word_col = np.array(word_col)\n",
    "cos_col = np.array(cos_col).reshape((-1, ))\n",
    "\n",
    "data = np.array([topic_col, word_col, cos_col]).T\n",
    "cosine_df = pd.DataFrame(data, columns=['topic', 'word', 'cosine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "108fd647-ac1f-4e51-b964-89d5aff7e66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286387, 286387, 286387)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_col), len(word_col), len(cos_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91a2dae2-65b0-4ddb-bbb7-bf9cb2af1583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>word</th>\n",
       "      <th>cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>symptom</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>1.0000000000000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>symptom</td>\n",
       "      <td>symptom</td>\n",
       "      <td>0.2028250618645076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>symptom</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>-0.10594274891158682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>symptom</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>0.4818992172118389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>symptom</td>\n",
       "      <td>angiotensin_converting_enzyme</td>\n",
       "      <td>0.21770178984822242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286382</th>\n",
       "      <td>symptom</td>\n",
       "      <td></td>\n",
       "      <td>0.11287630757556942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286383</th>\n",
       "      <td>symptom</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>-0.07515445695102974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286384</th>\n",
       "      <td>symptom</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>0.5731043226562862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286385</th>\n",
       "      <td>symptom</td>\n",
       "      <td>\\n</td>\n",
       "      <td>0.08848080349404483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286386</th>\n",
       "      <td>symptom</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>-0.07221144716700885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286387 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          topic                           word                cosine\n",
       "0       symptom                          [CLS]    1.0000000000000009\n",
       "1       symptom                        symptom    0.2028250618645076\n",
       "2       symptom                          [SEP]  -0.10594274891158682\n",
       "3       symptom                          [CLS]    0.4818992172118389\n",
       "4       symptom  angiotensin_converting_enzyme   0.21770178984822242\n",
       "...         ...                            ...                   ...\n",
       "286382  symptom                                  0.11287630757556942\n",
       "286383  symptom                          [SEP]  -0.07515445695102974\n",
       "286384  symptom                          [CLS]    0.5731043226562862\n",
       "286385  symptom                             \\n   0.08848080349404483\n",
       "286386  symptom                          [SEP]  -0.07221144716700885\n",
       "\n",
       "[286387 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2ea7e3e-1c0b-40ae-a87f-a3f4d733e936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>word</th>\n",
       "      <th>cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115202</th>\n",
       "      <td>symptom</td>\n",
       "      <td>role</td>\n",
       "      <td>0.714869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102915</th>\n",
       "      <td>symptom</td>\n",
       "      <td>noted</td>\n",
       "      <td>0.710099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104451</th>\n",
       "      <td>symptom</td>\n",
       "      <td>virus</td>\n",
       "      <td>0.710076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71683</th>\n",
       "      <td>symptom</td>\n",
       "      <td>guarding</td>\n",
       "      <td>0.683049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274634</th>\n",
       "      <td>symptom</td>\n",
       "      <td>fixed</td>\n",
       "      <td>0.661575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204116</th>\n",
       "      <td>symptom</td>\n",
       "      <td>code</td>\n",
       "      <td>0.657880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97283</th>\n",
       "      <td>symptom</td>\n",
       "      <td>risk</td>\n",
       "      <td>0.657662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54275</th>\n",
       "      <td>symptom</td>\n",
       "      <td>virus</td>\n",
       "      <td>0.657354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87044</th>\n",
       "      <td>symptom</td>\n",
       "      <td>better</td>\n",
       "      <td>0.656019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151483</th>\n",
       "      <td>symptom</td>\n",
       "      <td>reported</td>\n",
       "      <td>0.650768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          topic      word    cosine\n",
       "115202  symptom      role  0.714869\n",
       "102915  symptom     noted  0.710099\n",
       "104451  symptom     virus  0.710076\n",
       "71683   symptom  guarding  0.683049\n",
       "274634  symptom     fixed  0.661575\n",
       "204116  symptom      code  0.657880\n",
       "97283   symptom      risk  0.657662\n",
       "54275   symptom     virus  0.657354\n",
       "87044   symptom    better  0.656019\n",
       "151483  symptom  reported  0.650768"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_df[\"cosine\"] = pd.to_numeric(cosine_df[\"cosine\"])\n",
    "word_df = cosine_df[(cosine_df.word != \"[CLS]\") \n",
    "                    & (cosine_df.word != \"[SEP]\")\n",
    "                    & (cosine_df.word.str.len() > 3)\n",
    "                    & ~(cosine_df.word.isin(all_topics))].sort_values(by=['cosine'], ascending=False)\n",
    "word_df = word_df[word_df.word.str.isalnum()]\n",
    "word_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b2898dc-952d-44fe-a029-449920c99193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>word</th>\n",
       "      <th>cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115202</th>\n",
       "      <td>symptom</td>\n",
       "      <td>role</td>\n",
       "      <td>0.714869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102915</th>\n",
       "      <td>symptom</td>\n",
       "      <td>noted</td>\n",
       "      <td>0.710099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104451</th>\n",
       "      <td>symptom</td>\n",
       "      <td>virus</td>\n",
       "      <td>0.710076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71683</th>\n",
       "      <td>symptom</td>\n",
       "      <td>guarding</td>\n",
       "      <td>0.683049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274634</th>\n",
       "      <td>symptom</td>\n",
       "      <td>fixed</td>\n",
       "      <td>0.661575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204116</th>\n",
       "      <td>symptom</td>\n",
       "      <td>code</td>\n",
       "      <td>0.657880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97283</th>\n",
       "      <td>symptom</td>\n",
       "      <td>risk</td>\n",
       "      <td>0.657662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54275</th>\n",
       "      <td>symptom</td>\n",
       "      <td>virus</td>\n",
       "      <td>0.657354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87044</th>\n",
       "      <td>symptom</td>\n",
       "      <td>better</td>\n",
       "      <td>0.656019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151483</th>\n",
       "      <td>symptom</td>\n",
       "      <td>reported</td>\n",
       "      <td>0.650768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31748</th>\n",
       "      <td>symptom</td>\n",
       "      <td>rather</td>\n",
       "      <td>0.650729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274123</th>\n",
       "      <td>symptom</td>\n",
       "      <td>derived</td>\n",
       "      <td>0.649914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6147</th>\n",
       "      <td>symptom</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0.648954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264414</th>\n",
       "      <td>symptom</td>\n",
       "      <td>curve</td>\n",
       "      <td>0.647978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25091</th>\n",
       "      <td>symptom</td>\n",
       "      <td>bearing</td>\n",
       "      <td>0.646051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32260</th>\n",
       "      <td>symptom</td>\n",
       "      <td>massive</td>\n",
       "      <td>0.644248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276678</th>\n",
       "      <td>symptom</td>\n",
       "      <td>surveillance</td>\n",
       "      <td>0.644231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231710</th>\n",
       "      <td>symptom</td>\n",
       "      <td>responsible</td>\n",
       "      <td>0.643179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26627</th>\n",
       "      <td>symptom</td>\n",
       "      <td>because</td>\n",
       "      <td>0.641563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156082</th>\n",
       "      <td>symptom</td>\n",
       "      <td>symptoms</td>\n",
       "      <td>0.640370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218936</th>\n",
       "      <td>symptom</td>\n",
       "      <td>definite</td>\n",
       "      <td>0.639847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212292</th>\n",
       "      <td>symptom</td>\n",
       "      <td>that</td>\n",
       "      <td>0.639313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257261</th>\n",
       "      <td>symptom</td>\n",
       "      <td>come</td>\n",
       "      <td>0.639155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242442</th>\n",
       "      <td>symptom</td>\n",
       "      <td>contact</td>\n",
       "      <td>0.638874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31235</th>\n",
       "      <td>symptom</td>\n",
       "      <td>followed</td>\n",
       "      <td>0.638391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270546</th>\n",
       "      <td>symptom</td>\n",
       "      <td>globally</td>\n",
       "      <td>0.634669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132065</th>\n",
       "      <td>symptom</td>\n",
       "      <td>cases</td>\n",
       "      <td>0.629520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220469</th>\n",
       "      <td>symptom</td>\n",
       "      <td>less</td>\n",
       "      <td>0.628914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194919</th>\n",
       "      <td>symptom</td>\n",
       "      <td>regard</td>\n",
       "      <td>0.628654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250618</th>\n",
       "      <td>symptom</td>\n",
       "      <td>digest</td>\n",
       "      <td>0.627749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          topic          word    cosine\n",
       "115202  symptom          role  0.714869\n",
       "102915  symptom         noted  0.710099\n",
       "104451  symptom         virus  0.710076\n",
       "71683   symptom      guarding  0.683049\n",
       "274634  symptom         fixed  0.661575\n",
       "204116  symptom          code  0.657880\n",
       "97283   symptom          risk  0.657662\n",
       "54275   symptom         virus  0.657354\n",
       "87044   symptom        better  0.656019\n",
       "151483  symptom      reported  0.650768\n",
       "31748   symptom        rather  0.650729\n",
       "274123  symptom       derived  0.649914\n",
       "6147    symptom       unknown  0.648954\n",
       "264414  symptom         curve  0.647978\n",
       "25091   symptom       bearing  0.646051\n",
       "32260   symptom       massive  0.644248\n",
       "276678  symptom  surveillance  0.644231\n",
       "231710  symptom   responsible  0.643179\n",
       "26627   symptom       because  0.641563\n",
       "156082  symptom      symptoms  0.640370\n",
       "218936  symptom      definite  0.639847\n",
       "212292  symptom          that  0.639313\n",
       "257261  symptom          come  0.639155\n",
       "242442  symptom       contact  0.638874\n",
       "31235   symptom      followed  0.638391\n",
       "270546  symptom      globally  0.634669\n",
       "132065  symptom         cases  0.629520\n",
       "220469  symptom          less  0.628914\n",
       "194919  symptom        regard  0.628654\n",
       "250618  symptom        digest  0.627749"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89a280cc-1ce6-4105-ad79-07f72f73e318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>word</th>\n",
       "      <th>cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>974483</th>\n",
       "      <td>ebola</td>\n",
       "      <td>role</td>\n",
       "      <td>0.714869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962196</th>\n",
       "      <td>ebola</td>\n",
       "      <td>noted</td>\n",
       "      <td>0.710099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963732</th>\n",
       "      <td>ebola</td>\n",
       "      <td>virus</td>\n",
       "      <td>0.710076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930964</th>\n",
       "      <td>ebola</td>\n",
       "      <td>guarding</td>\n",
       "      <td>0.683049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133915</th>\n",
       "      <td>ebola</td>\n",
       "      <td>fixed</td>\n",
       "      <td>0.661575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078429</th>\n",
       "      <td>ebola</td>\n",
       "      <td>other</td>\n",
       "      <td>-0.140917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116801</th>\n",
       "      <td>ebola</td>\n",
       "      <td>hand</td>\n",
       "      <td>-0.140965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970936</th>\n",
       "      <td>ebola</td>\n",
       "      <td>other</td>\n",
       "      <td>-0.146952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119358</th>\n",
       "      <td>ebola</td>\n",
       "      <td>case</td>\n",
       "      <td>-0.149871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116800</th>\n",
       "      <td>ebola</td>\n",
       "      <td>other</td>\n",
       "      <td>-0.154031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112320 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic      word    cosine\n",
       "974483   ebola      role  0.714869\n",
       "962196   ebola     noted  0.710099\n",
       "963732   ebola     virus  0.710076\n",
       "930964   ebola  guarding  0.683049\n",
       "1133915  ebola     fixed  0.661575\n",
       "...        ...       ...       ...\n",
       "1078429  ebola     other -0.140917\n",
       "1116801  ebola      hand -0.140965\n",
       "970936   ebola     other -0.146952\n",
       "1119358  ebola      case -0.149871\n",
       "1116800  ebola     other -0.154031\n",
       "\n",
       "[112320 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df[(word_df.topic == \"ebola\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae5183-6872-455e-8565-d0a0954c2211",
   "metadata": {},
   "source": [
    "**Scratch Work (aka ignore)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16d44256-ae23-450a-8cf7-1445fcf092f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ang', '##iot', '##ens', '##in', '_', 'converting', '_', 'enzyme', '2', '(', 'ace', '##2', ')', 'as', 'a', 'sar', '##s', '_', 'co', '##v', '_', '2', 'receptor', 'molecular', '_', 'mechanisms', 'and', 'potential', 'therapeutic', '_', 'target', 'sar', '##s', '_', 'co', '##v', '_', '2', 'has', 'been', 'sequence', '##d', '3', '.', 'a', 'phylogenetic', 'analysis', '3', ',', '4', 'found', 'a', 'bat', 'origin', 'for', 'the', 'sar', '##s', '_', 'co', '##v', '_', '2', '.', 'there', 'is', 'a', 'diversity', 'of', 'possible', 'intermediate', '_', 'hosts', 'for', 'sar', '##s', '_', 'co', '##v', '_', '2', ',', 'including', 'pang', '##olin', '##s', ',', 'but', 'not', 'mice', 'and', 'rats', '5', '.', 'there', 'are', 'many', 'similarities', 'of', 'sar', '##s', '_', 'co', '##v', '_', '2', 'with', 'the', 'original', 'sar', '##s', 'co', '##v', '.', 'using', 'computer', 'modeling', ',', 'xu', 'et', 'al', '.', '6', 'found', 'that', 'the', 'spike', '_', 'proteins', 'of', 'sar', '##s', '_', 'co', '##v', '_', '2', 'and', 'sar', '##s', 'co', '##v', 'have', '[SEP]']\n",
      "0 : not found\n",
      "512 : not found\n",
      "1024 : not found\n",
      "1536 : not found\n",
      "2048 : not found\n",
      "2560 : not found\n",
      "3072 : not found\n",
      "3584 : not found\n",
      "4096 : not found\n",
      "4608 : not found\n",
      "5120 : not found\n",
      "5632 : not found\n",
      "6144 : not found\n",
      "6656 : not found\n",
      "7168 : not found\n",
      "7680 : not found\n",
      "8192 : not found\n",
      "8704 : not found\n",
      "9216 : not found\n"
     ]
    }
   ],
   "source": [
    "target_word_embeddings = []\n",
    "\n",
    "for i in np.arange(0, len(text[0]), 512):\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text[0][i:min(i+512, len(text[0]))], tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "\n",
    "    word = \"covid_19\" # virus_type[1]\n",
    "\n",
    "    # Find the position 'bank' in list of tokens\n",
    "    if word not in tokenized_text:\n",
    "        if i == 0:\n",
    "            print(tokenized_text)\n",
    "        print(i, \": not found\")\n",
    "    else:\n",
    "        print(i, \": found\")\n",
    "        word_index = tokenized_text.index(word)\n",
    "        # Get the embedding for bank\n",
    "        word_embedding = list_token_embeddings[word_index]\n",
    "        target_word_embeddings.append(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b93ebe3-14b6-4199-be91-431f70397e84",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▍                              | 1328/29500 [5:54:55<125:29:12, 16.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     word_embeddings \u001b[38;5;241m=\u001b[39m list_token_embeddings\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     word_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_token_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/schemaegc/lib/python3.8/site-packages/numpy/lib/function_base.py:5444\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     values \u001b[38;5;241m=\u001b[39m ravel(values)\n\u001b[1;32m   5443\u001b[0m     axis \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word_embeddings = None # shape: (# words, 768)\n",
    "all_words = []\n",
    "add_topics = True\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "    split_text = doc.split(\".\") # split corpus into list of sentences\n",
    "    if add_topics:\n",
    "        split_text[:0] = all_topics\n",
    "        add_topics = False\n",
    "\n",
    "    for sentence in split_text:\n",
    "        tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)\n",
    "        list_token_embeddings = np.array(get_bert_embeddings(tokens_tensor, segments_tensors, model))[1:-1] # shape: (# words in sentence, 768)\n",
    "        all_words.extend(tokenized_text[1:-1])\n",
    "\n",
    "        if word_embeddings is None:\n",
    "            word_embeddings = list_token_embeddings\n",
    "        else:\n",
    "            word_embeddings = np.append(word_embeddings, list_token_embeddings, axis=0)\n",
    "    \n",
    "    # word_embeddings.append(list_token_embeddings[1:-1]) # ignore CLS and SEP tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "670c7c83-84c0-42d8-bf40-fe1a6685cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the distance between the embeddings of the word in all the given contexts of the word\n",
    "\n",
    "list_of_distances = []\n",
    "for text1, embed1 in zip(split_text, target_word_embeddings):\n",
    "    for text2, embed2 in zip(split_text, target_word_embeddings):\n",
    "        cos_dist = 1 - cosine(embed1, embed2)\n",
    "        list_of_distances.append([text1, text2, cos_dist])\n",
    "\n",
    "distances_df = pd.DataFrame(list_of_distances, columns=['text1', 'text2', 'distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e7fe99b-6079-45c2-939c-ed76a1064199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sars</td>\n",
       "      <td>sars</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sars</td>\n",
       "      <td>angiotensin_converting_enzyme 2 ( ace2 ) as a ...</td>\n",
       "      <td>0.615406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sars</td>\n",
       "      <td>there is a diversity of possible intermediate...</td>\n",
       "      <td>0.602366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sars</td>\n",
       "      <td>5 identity in amino_acid sequences 6 and , im...</td>\n",
       "      <td>0.547345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sars</td>\n",
       "      <td>this similarity with sars cov is critical bec...</td>\n",
       "      <td>0.531670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sars</td>\n",
       "      <td>it is required for host_cell entry and subseq...</td>\n",
       "      <td>0.527674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sars</td>\n",
       "      <td>wan et al</td>\n",
       "      <td>0.520226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sars</td>\n",
       "      <td>4 reported that residue 394 ( glutamine ) in ...</td>\n",
       "      <td>0.517760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sars</td>\n",
       "      <td>thus , the sars_cov_2 spike_protein was predi...</td>\n",
       "      <td>0.512084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sars</td>\n",
       "      <td>further analysis even suggested that sars_cov...</td>\n",
       "      <td>0.509995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text1                                              text2  distance\n",
       "0   sars                                               sars  1.000000\n",
       "1   sars  angiotensin_converting_enzyme 2 ( ace2 ) as a ...  0.615406\n",
       "3   sars   there is a diversity of possible intermediate...  0.602366\n",
       "9   sars   5 identity in amino_acid sequences 6 and , im...  0.547345\n",
       "14  sars   this similarity with sars cov is critical bec...  0.531670\n",
       "15  sars   it is required for host_cell entry and subseq...  0.527674\n",
       "10  sars                                         wan et al   0.520226\n",
       "11  sars   4 reported that residue 394 ( glutamine ) in ...  0.517760\n",
       "13  sars   thus , the sars_cov_2 spike_protein was predi...  0.512084\n",
       "12  sars   further analysis even suggested that sars_cov...  0.509995"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df = distances_df[distances_df.text1 == word].sort_values(by=['distance'], ascending=False)\n",
    "word_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0b7bc71-0427-451f-a856-a5b5cfe46a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' a phylogenetic analysis 3 , 4 found a bat origin for the sars_cov_2 '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df[\"text2\"][2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schemaegc",
   "language": "python",
   "name": "schemaegc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
